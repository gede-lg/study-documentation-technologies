# Models

**Título da Explicação:** Explorando o elemento **Models** no Docker Compose

---

## Introdução

O elemento top-level `models` do Docker Compose permite declarar dependências de modelos de IA (artifacts OCI) dentro do seu arquivo de composição, integrando-os a um *model runner* que expõe uma API consumível pelos serviços. Essa funcionalidade foi introduzida na versão 2.38.0 do Docker Compose, possibilitando a orquestração não apenas de containers, mas também de inferência de modelos de aprendizado de máquina diretamente via Compose ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

## Sumário

1. [Conceitos Fundamentais](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#conceitos-fundamentais)
2. [Sintaxe Detalhada e Uso Prático](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#sintaxe-detalhada-e-uso-pr%C3%A1tico)
3. [Cenários de Restrição ou Não Aplicação](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#cen%C3%A1rios-de-restri%C3%A7%C3%A3o-ou-n%C3%A3o-aplica%C3%A7%C3%A3o)
4. [Componentes Chave Associados](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#componentes-chave-associados)
5. [Melhores Práticas e Padrões de Uso](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#melhores-pr%C3%A1ticas-e-padr%C3%B5es-de-uso)
6. [Exemplo Prático Completo](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#exemplo-pr%C3%A1tico-completo)
7. [Sugestões para Aprofundamento](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#sugest%C3%B5es-para-aprofundamento)

---

## Conceitos Fundamentais

- **Modelos como OCI Artifacts:** Cada entrada em `models` aponta para um identificador OCI (`model`), que será puxado do registro e executado pelo *model runner* embutido no Docker Compose ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).
- **Isolamento e Conectividade:** O Compose injeta automaticamente variáveis de ambiente (como `AI_<MODEL>_URL`) nos containers dos serviços que referenciam esses modelos, fornecendo a URL de acesso à API de inferência.
- **Separação de Responsabilidades:** Serviços só acessam os modelos que forem explicitamente listados na seção `models` dentro de cada serviço, reforçando o princípio de privilégio mínimo ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

---

## Sintaxe Detalhada e Uso Prático

### 1. Declaração Top-Level

```yaml
# Seção global de modelos
models:
  ai_model:
    model: ai/model:latest
    context_size: 2048
    runtime_flags:
      - "--batch-size=1"
      - "--optimize"

```

- `ai_model`: nome lógico do modelo (referenciado pelos serviços).
- `model` (obrigatório): identificador OCI (imagem) do modelo.
- `context_size`: tamanho máximo de contexto em tokens.
- `runtime_flags`: flags passadas diretamente ao mecanismo de inferência ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

### 2. Referência no Serviço

### 2.1. Sintaxe Curta

```yaml
services:
  chat-app:
    image: my-chat-app:1.0
    models:
      - ai_model

```

- Indica que `chat-app` consome `ai_model` usando o nome definido em `models`.

### 2.2. Sintaxe Longa

```yaml
services:
  chat-app:
    image: my-chat-app:1.0
    models:
      ai_model:
        endpoint_var: CHAT_MODEL_URL

```

- Permite customizar a variável de ambiente (`endpoint_var`) que receberá a URL do modelo ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

---

## Cenários de Restrição ou Não Aplicação

- **Versões Antigas:** O atributo `models` só está disponível no Docker Compose ≥ 2.38.0. Em versões anteriores, o Compose simplesmente ignoraria essa seção ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).
- **Aplicações sem IA:** Para stacks que não requerem inferência de modelos de ML/IA, adicionar a seção `models` não traz benefícios e pode aumentar a complexidade do deploy.
- **Ambientes sem Model Runner:** Se o ambiente alvo não suportar um *model runner* compatível, os containers não conseguirão consumir os modelos.

---

## Componentes Chave Associados

| Componente | Descrição |
| --- | --- |
| `model` | Identificador OCI do modelo (ex.: `ai/smollm2:latest`). É o ponto de partida para pull/run. |
| `context_size` | (Opcional) Define quantos tokens de contexto o modelo aceita em cada inferência. |
| `runtime_flags` | (Opcional) Lista de flags de linha de comando para passar ao serviço de inferência. |
| `endpoint_var` | (Serviço) Define o nome da variável de ambiente que expõe a URL do modelo dentro do container. |

---

## Melhores Práticas e Padrões de Uso

1. **Versionamento de Artefatos:** Fixe a tag do modelo (`model: ai/model:v1.2.3`) para garantir reprodutibilidade de inferências.
2. **Nomes Significativos:** Utilize nomes claros em `models`, refletindo o propósito (ex.: `sentiment_analyzer`, `nlp_summarizer`).
3. **Limitação de Contexto:** Ajuste `context_size` de acordo com as especificações do modelo para evitar erros de token overflow.
4. **Flags Otimizadas:** Use `runtime_flags` para habilitar recursos de aceleração ou profiling, sem poluir a seção `build`.
5. **Ambientes de Teste vs Produção:** Utilize múltiplos arquivos Compose (`docker-compose.yml` + `docker-compose.prod.yml`) para alternar entre modelos de teste e de produção por override.

---

## Exemplo Prático Completo

```yaml
version: '3.9'

services:
  web:
    image: my-web-app:2.0
    ports:
      - "8080:80"
    models:
      summarizer:
        endpoint_var: SUMMARIZER_URL

  inference:
    image: inference-client:latest
    depends_on:
      - model-runner
    command: ["python", "client.py", "--model-url", "${SUMMARIZER_URL}"]

models:
  summarizer:
    model: ai/hf-summarizer:1.0.0
    context_size: 512
    runtime_flags:
      - "--threads=4"
      - "--max-seq-len=512"

# Ao executar:
# $ docker compose up
# - model-runner puxará o OCI ai/hf-summarizer:1.0.0
# - injetará SUMMARIZER_URL nos containers web e inference
# - clients enviarão requisições de inferência de texto ao model-runner

```

Esse setup define um serviço web que expõe uma interface e um cliente de inferência separado, ambos consumindo o modelo `summarizer`. O *model runner* orquestrado pelo Compose gerencia o ciclo de vida do modelo e fornece escalabilidade e isolamento.

---

## Sugestões para Aprofundamento

- Explore os [**AI/ML samples**](https://docs.docker.com/samples/ai-ml/) do Docker para projetos de exemplo com Compose ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).
- Consulte a [**Compose Specification**](https://github.com/compose-spec/compose-spec) para entender outros elementos avançados de orquestração.
- Analise integrações com frameworks como **Seldon Core** ou **KServe** para pipelines de ML em Kubernetes via Compose Bridge.