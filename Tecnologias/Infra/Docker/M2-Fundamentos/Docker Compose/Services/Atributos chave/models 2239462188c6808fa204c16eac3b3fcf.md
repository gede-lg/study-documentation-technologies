# models

**Entendendo o atributo `models` em serviços no Docker Compose**

---

## Introdução

O Docker Compose 2.38.0 (e superiores) introduziu o suporte nativo a **modelos de IA** (AI models) como artefatos OCI que podem ser puxados, executados por um “model runner” e expostos como APIs para containers de serviço. Para que um serviço tenha acesso a esses modelos, é necessário declarar explicitamente um atributo `models` dentro da definição do serviço no arquivo Compose ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

---

## Sumário

1. [Conceitos Fundamentais](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#conceitos-fundamentais)
2. [Sintaxe Detalhada e Uso Prático](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#sintaxe-detalhada-e-uso-pr%C3%A1tico)
    - Sintaxe curta
    - Sintaxe longa
3. [Cenários de Restrição ou Não Aplicação](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#cen%C3%A1rios-de-restri%C3%A7%C3%A3o-ou-n%C3%A3o-aplica%C3%A7%C3%A3o)
4. [Componentes-chave Associados](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#componentes-chave-associados)
5. [Melhores Práticas e Padrões de Uso](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#melhores-pr%C3%A1ticas-e-padr%C3%B5es-de-uso)
6. [Exemplo Prático Completo](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#exemplo-pr%C3%A1tico-completo)
7. [Sugestões para Aprofundamento](https://chatgpt.com/c/6859817f-01e8-8013-9e5a-cbb14e4c689f#sugest%C3%B5es-para-aprofundamento)

---

## Conceitos Fundamentais

Um arquivo Docker Compose deve declarar um elemento de nível superior `services`, cujas chaves representam nomes de serviço e cujos valores são definições de configuração para cada container ([docs.docker.com](https://docs.docker.com/reference/compose-file/services/?utm_source=chatgpt.com)).

Além disso, a especificação define um elemento de nível superior `models`, onde você lista artefatos OCI de IA (modelos), informando a origem (identificador do artefato), tamanho de contexto e flags de runtime. Para que um serviço possa usar um desses modelos, você precisa referenciá-lo no atributo `models` da definição do próprio serviço ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

---

## Sintaxe Detalhada e Uso Prático

### 1. Sintaxe Curta

Quando você só precisa conceder acesso simples a um ou mais modelos, use uma lista:

```yaml
services:
  app:
    image: app                              # Imagem do seu serviço
    models:
      - ai_model                            # Acesso ao modelo 'ai_model'

models:
  ai_model:
    model: ai/model                         # Identificador OCI do modelo

```

- **Como funciona:** o Compose injeta, por padrão, uma variável de ambiente `AI_MODEL_URL` no container, apontando para a URL fornecida pelo model runner ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

### 2. Sintaxe Longa

Se você precisar personalizar o nome da variável de ambiente ou outros parâmetros, utilize mapeamento:

```yaml
services:
  app:
    image: app
    models:
      my_model:                             # Nome da referência ao modelo
        endpoint_var: MODEL_URL            # Nome da env var a ser injetada

models:
  my_model:
    model: ai/model                        # Identificador OCI
    context_size: 1024                     # Token context size
    runtime_flags:                         # Flags passadas ao runtime
      - "--a-flag"
      - "--another-flag=42"

```

- **Explicação:**
    - `endpoint_var` define a variável de ambiente dentro do container (aqui `MODEL_URL`).
    - `context_size` limita o tamanho do contexto de tokens usado pelo modelo.
    - `runtime_flags` permite passar flags brutas ao motor de inferência ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

---

## Cenários de Restrição ou Não Aplicação

- **Versão mínima:** somente disponível em Compose CLI ≥ 2.38.0; versões mais antigas ignoram o atributo `models` (avisam ou descartam o campo) ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/), [matsuand.github.io](https://matsuand.github.io/docs.docker.jp.onthefly/compose/compose-file/?utm_source=chatgpt.com)).
- **Implementações parciais:** ambientes que não suportem “model runner” nativamente (ex.: ciertos Orchestrators) vão simplesmente **ignorar** essa seção.
- **Sem casos de IA:** se seu aplicativo não requer inferência de modelos, o uso do atributo é desnecessário e sobrecarrega o YAML.

---

## Componentes-chave Associados

- **`services`**: seção principal que define cada serviço (container).
- **`models`** (no `services`): lista ou mapa de referências a modelos definidos no nível superior.
- **`models`** (top-level): mapeamento com chaves que representam modelos, cada uma contendo:
    - `model` (obrigatório): identificador OCI (string).
    - `context_size` (opcional): número inteiro de tokens.
    - `runtime_flags` (opcional): lista de strings com flags de linha de comando.
- **`endpoint_var`**: (somente na sintaxe longa em `services`) define a variável de ambiente usada pelo serviço para acessar o modelo ([docs.docker.com](https://docs.docker.com/reference/compose-file/models/)).

---

## Melhores Práticas e Padrões de Uso

- **Fixar versões:** utilize digests (`ai/model@sha256:…`) para garantir reprodutibilidade.
- **Isolar acesso:** somente conceda a serviços que realmente façam inferência, evitando exposição desnecessária.
- **Dimensionar contexto:** ajuste `context_size` conforme as necessidades do modelo para balancear desempenho e consumo de memória.
- **Flags de runtime:** explore `runtime_flags` para otimizar parâmetros de inferência (batch size, threads, etc.).
- **Fallbacks e monitoração:** implemente healthchecks no serviço que consome o modelo para lidar com falhas do model runner.

---

## Exemplo Prático Completo

Imagine um serviço Python que realiza análise de sentimento usando um modelo Hugging Face publicado como artefato OCI.

```yaml
# docker-compose.yml
version: "3.9"

services:
  sentiment:
    image: python:3.11-slim
    container_name: sentiment_app
    # Concede acesso ao modelo definido abaixo
    models:
      sentiment_model:
        endpoint_var: SENTIMENT_URL    # Variável injetada no container
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      - ./app:/usr/src/app
    working_dir: /usr/src/app
    command: >
      bash -c "pip install -r requirements.txt &&
               python analyze_sentiment.py"

models:
  sentiment_model:
    model: huggingface/sentiment:1.0   # Artefato OCI do modelo
    context_size: 2048
    runtime_flags:
      - "--batch=4"
      - "--optimize_latency"

```

Dentro de `analyze_sentiment.py`, basta ler a variável `SENTIMENT_URL` para saber onde conectar-se ao serviço de inferência.

---

## Sugestões para Aprofundamento

- **Compose Specification** (GitHub): explore [https://github.com/compose-spec/compose-spec](https://github.com/compose-spec/compose-spec) para ver a definição formal.
- **Model Runner**: investigue projetos de “model runner” compatíveis (ex.: Docker Model Runner).
- **Compose Bridge**: veja como converter Compose para Kubernetes e suporte a modelos.
- **Tutoriais de IA com Docker**: pesquise exemplos práticos de deploy de LLMs via Docker Compose.

---

> Com essa visão detalhada, você poderá aproveitar o atributo models para integrar modelos de IA de forma segura, versionada e eficiente em seus ambientes Docker Compose.
>